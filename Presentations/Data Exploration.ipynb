{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Visualisation and Exploration\n",
    "\n",
    "## Applied Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "-  Recap and Outlook\n",
    "\n",
    "\n",
    "- Descriptive Statistics\n",
    "    - Univariate Analysis\n",
    "    - Measures of Central Tendency\n",
    "    - Moments\n",
    "    - Measures of Shape\n",
    "    - Transforms\n",
    "    - Density Estimation\n",
    "    - Multivariate Analysis\n",
    "- Dimensionality Reduction\n",
    "    - Linear Dimensionality Reduction\n",
    "    - Nonlinear Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Descriptive vs Inferential Statistics\n",
    "-  Inferential/Inductive:\n",
    "    - Summarise sample\n",
    "    - Based on probability theory\n",
    "- Descriptive:\n",
    "    - Describe or summarise dataset\n",
    "    - Useful for assessing data quality, developing models, general understanding\n",
    "    - Often presented alongside conclusions inferential statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Univariate Measures\n",
    "- Central tendency\n",
    "    - Mean, median, mode\n",
    "- Variability\n",
    "    - Variance, quartiles, min, max, kurtosis, skewness\n",
    "- Graphical/Tabular format\n",
    "    - Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of Variable\n",
    "- Categorical/Discrete (Qualitative)\n",
    "    - Nominal\n",
    "    - Ordinal: ordered\n",
    "    - Dichotomous: only 2 categories\n",
    "- Continuous (Quantitative)\n",
    "    - Interval: measured along a continuum and numerical value\n",
    "        - e.g. temperature measured in degrees Celsius or Fahrenheit). So the difference between 20C and 30C is the same as 30C to 40C\n",
    "    - Ratio: added condition that 0 indicates that there is none of that variable\n",
    "        - e.g. temperature measured in Kelvin, as 0 Kelvin indicates that there is “no temperature”\n",
    "        - e.g. height, mass, distance etc.\n",
    "        - reflects the fact that you can use the ratio of measurements (distance of 10 m is twice the distance of 5 m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Central Tendency\n",
    "\n",
    "- Mean (arithmetic) $\\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i$\n",
    "    - can be used with both discrete and continuous values\n",
    "    - particularly susceptible to the influence of outliers\n",
    "\n",
    "|staff  |1    |2    |3    |4    |5    |6    |7    |8    |9    |10   |\n",
    "|-------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "|salary |15k  |18k  |16k  |14k  |15k  |15k  |12k  |17k  |90k  |95k  |\n",
    "\n",
    "    - mean: 30.7k, most in the 12k to 18k range\n",
    "    - mean is being skewed by the two large salaries\n",
    "- Median\n",
    "    - middle score for a set of data after sorting\n",
    "    - less affected by outliers and skewed data\n",
    "- Mode\n",
    "    - most frequent value (discrete)\n",
    "    - highest bar in histogram (continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mode Examples\n",
    "\n",
    "![alt text](resources/mode-1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mode Examples\n",
    "\n",
    "![alt text](resources/mode-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mode Examples\n",
    "\n",
    "![alt text](resources/mode-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mode Examples\n",
    "\n",
    "![alt text](resources/mode-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skewed Distributions and the Mean and Median (Normal Distrubution)\n",
    "\n",
    "![alt text](resources/skewed-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skewed Distributions and the Mean and Median (Skewed Distrubution)\n",
    "\n",
    "![alt text](resources/skewed-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When to use the mean, median and mode\n",
    "\n",
    "|Type of Variable           |Best measure of central tendency |\n",
    "|---------------------------|---------------------------------|\n",
    "|Nominal                    |Mode                             |\n",
    "|Ordinal                    |Mean                             |\n",
    "|Interval/Ratio(not skewed) |Mean                             |\n",
    "|Interval/Ratio(skewed)     |Median                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measure of Dispersion\n",
    "\n",
    "- Range: man() - min()\n",
    "    - Only depends on 2 values\n",
    "    - Most useful in representing the dispersion of small data sets\n",
    "- Variance: $Var(X) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\mu)^2$\n",
    "- Unbaised Variance: $Var(X) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\mu)^2$\n",
    "- Standard devaition $\\sigma = \\sqrt{Var(x)}$\n",
    "- Quantiles\n",
    "    - Cut-points dividing the range of a probability distribution into contiguous intervals with equal probabilities\n",
    "    - or dividing the observations in a sample in the same way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unbaised Variance: Intuition \n",
    "\n",
    "$$\\begin{array}{c}\n",
    "\\mbox{The degree to which}\\\\\n",
    "x_i\\mbox{ varies from }\\bar{X}\n",
    "\\end{array}+\\begin{array}{c}\n",
    "\\mbox{The degree to which}\\\\\n",
    "\\bar{X}\\mbox{ varies from }\\mu\n",
    "\\end{array}=\\begin{array}{c}\n",
    "\\mbox{The degree to which }\\\\\n",
    "x_i\\mbox{ varies from }\\mu.\n",
    "\\end{array}$$\n",
    "\n",
    "That is, \n",
    "\n",
    "$$E\\left[\\left(X-\\bar{X}\\right)^{2}\\right]+E\\left[\\left(\\bar{X}-\\mu\\right)^{2}\\right]=E\\left[\\left(X-\\mu\\right)^{2}\\right].$$\n",
    "\n",
    "Proof requires a bit of algebra. But assuming it is true, we can rearrange to get: \n",
    "\n",
    "$$E\\left[\\left(X-\\bar{X}\\right)^{2}\\right]=\\underset{\\sigma^{2}}{\\underbrace{E\\left[\\left(X-\\mu\\right)^{2}\\right]}}-\\underset{\\frac{\\sigma^{2}}{n}}{\\underbrace{E\\left[\\left(\\bar{X}-\\mu\\right)^{2}\\right]}}=\\frac{n-1}{n}\\sigma^2.$$\n",
    "\n",
    "See [wikipedia page](https://en.wikipedia.org/wiki/Bessel's_correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quantiles\n",
    "\n",
    "- $k^{th} q$-quantile is where the cumulative distribution function crosses $\\frac{k}{q}$\n",
    "- $Pr[X \\geq x] \\geq 1 - \\frac{k}{q}$\n",
    "- The only 2-quantile is called the median\n",
    "- The 4-quantiles are called quartiles $Q$; the difference between upper and lower quartiles is also called the interquartile range, mid-spread or middle fifty $IQR = Q_3 - Q_1$\n",
    "- The 100-quantiles are called percentiles $P$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Raw Moments\n",
    "\n",
    "Let $X$ be a random variable with a probability distribution $P$ and mean value $\\mu = E[X]$ (i.e. the first raw moment or moment about zero), the operator $E$ denoting the expected value of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Central Moments\n",
    "\n",
    "- $n^{th}$ central moment\n",
    "\n",
    "$$ mu_n = E \\left[(X - E[X])^n\\right]$$\n",
    "\n",
    "$\\mu_0$ $E \\left[(X - E[X])^0\\right] = 1$\n",
    "$$$$ $\\mu_1$ $E \\left[(X - E[X])^1\\right] = 0$\n",
    "$$$$ $\\mu_2$ $E \\left[(X - E[X])^2\\right] = \\sigma^2$ (variance)\n",
    "\n",
    "- The third and fourth central moments are used to define the standardisedmoments which are used to define skewness and kurtosis, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Standardised Moments\n",
    "Ratio of the $k^{th}$ moment about the mean and the standard deviation to the power of $k$\n",
    "\n",
    "$$ \\bar{\\mu_k} = \\frac{\\mu_k}{\\sigma^k} = \\frac{E\\left[(X - \\mu)^k\\right]}{\\left(\\sqrt{E\\left[(X - \\mu)^2\\right]}\\right)^k} $$\n",
    "\n",
    "power of $k$ is because moments scale as $x^k$ → scale invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cumulants\n",
    "- Alternative to moments, some useful properties\n",
    "- Relation to moments:\n",
    "\n",
    "|Cumulant|Raw moment                                            |Central          |Standardised|\n",
    "|--------|------------------------------------------------------|-----------------|------------|\n",
    "|$k_1$   |$\\mu_1$                                               |0                |0           |\n",
    "|$k_2$   |$\\mu_2 -\\mu^2_1$                                      |$\\mu_2$          |1           |\n",
    "|$k_3$   |$\\mu_3-3\\mu_2\\mu_1+2\\mu^3_1$                          |$\\mu_3$          |$\\mu_3$     |\n",
    "|$k_4$   |$\\mu_4-4\\mu_3\\mu_1-3\\mu^2_2+12\\mu_2\\mu^2_1-6\\mu^4_1$  |$\\mu_4-3\\mu^2_2$ |$\\mu_4-3$   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measures of Shape: Skewness\n",
    "- “lopsidedness” of the distribution\n",
    "- symmetric distributions (if defined) = 0\n",
    "\n",
    "$$Skew[X]\n",
    "        = E \\left[\\left({\\frac {X-\\mu }{\\sigma }}\\right)^3\\right] \n",
    "        = \\frac{\\mu_3}{\\sigma^3} \n",
    "        = {\\frac {E \\left[(X-\\mu)^3\\right]}{(E \\left[(X-\\mu )^2\\right])^{3/2}}}\n",
    "        = {\\frac {\\kappa_3}{\\kappa_2^{3/2}}}\n",
    "$$\n",
    "\n",
    "- where:\n",
    "    - $\\mu_3$: third central moment\n",
    "    - $k^t$: $t^{th}$ cumulants\n",
    "- Sample skewness:\n",
    "$$b_1={\\frac {m_3}{s^3}} = \\frac{{\\tfrac{1}{n}}\\sum_{i=1}^n(x_i - \\bar{x})^3}{\\left[\\tfrac{1}{n-1}\\sum _{i=1}^n(x_i-\\bar{x})^2\\right]^{3/2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measures of Shape: Kurtosis\n",
    "\n",
    "- heaviness of the tail of the distribution, compared to the normal distribution of the same variance\n",
    "- always positive\n",
    "- kurtosis of a normal is $3\\sigma^4 \\implies $ \\alert{excess kurtosis} $= Kurt[X] - 3$\n",
    "$$ Kurt[X] \n",
    "        = E \\left[\\left({\\frac {X-\\mu }{\\sigma }}\\right)^4\\right] \n",
    "        = \\frac{\\mu_4}{\\sigma^4} \n",
    "        = \\frac{E[(X-\\mu)^4]}{(E[(X-\\mu )^2])^2}\n",
    "        $$\n",
    "- Sample excess kurtosis:\n",
    "$$g_2 = \\frac{m_4}{m_2^2} - 3 = \\frac{{\\tfrac{1}{n}} \\sum_{i=1}^n(x_i-\\bar{x})^4}{\\left({\\tfrac{1}{n}}\\sum_{i=1}^n(x_i - \\bar{x})^2\\right)^2} - 3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![alt text](resources/skewed-kurtosed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variance Stabilising Transforms\n",
    "- Usual assumption in regression is that the variance of each observation is the same\n",
    "- Problem: In many cases, the variance is not constant, but is related to the mean\n",
    "    - Poisson Data (Counts of events): $E(X) = Var(X) = \\mu$\n",
    "    - Binomial Data (and Percents):  $E(X) = m\\pi, \\quad Var(X) = m\\pi(1 - \\pi)$\n",
    "    - General Case: $E(X) = \\mu, \\quad Var(X) = f(\\mu)$\n",
    "    - Power relationship: $Var(X) = \\sigma^2 = \\alpha^2 \\mu^{2 \\beta}$\n",
    "\n",
    "$$\\sigma = \\alpha \\mu^\\beta \\implies \\log(\\sigma) = \\log(\\alpha) + \\beta \\log(\\mu)$$\n",
    "\n",
    "- Box-Cox transformation (Sakia, 1992) can be used to diagnose and transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### De-correlating\n",
    "\n",
    "Transform data so that it has diagonal covariance matrix ${\\bf \\Sigma} = {\\bf X}{\\bf X}^T$. This transform can be found by solving an eigenvalue problem:\n",
    "\n",
    " $${\\bf \\Sigma}{\\bf \\Phi} = {\\bf \\Phi} {\\bf \\Lambda}$$\n",
    "\n",
    "where ${\\bf \\Lambda}$ is a diagonal matrix of eigenvalues, and the columns of ${\\bf \\Phi}$ are the eigenvectors of the covariance matrix.\n",
    "\n",
    "$\\therefore {\\bf \\Phi}$ \\alert{diagonalises} ${\\bf \\Sigma}$. \n",
    "\n",
    "We can also write the diagonalised covariance as:\n",
    "\n",
    " $${\\bf \\Phi}^T {\\bf \\Sigma} {\\bf \\Phi} = {\\bf \\Lambda} \\quad (1)$$\n",
    "\n",
    "So to de-correlate a single vector ${\\bf x}_i$ (e.g. at test time), we do:\n",
    "\n",
    " $$\\hat{\\bf x}_i = {\\bf \\Phi}^T {\\bf x}_i \\quad (2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Whitening\n",
    "\n",
    "- The diagonal elements (eigenvalues) in ${\\bf \\Lambda}$ may be the same or different\n",
    "- If we make them all the same, then this is called \\alert{whitening} the data\n",
    "- Each eigenvalue determines the length of its associated eigenvector\n",
    "- Not whitened $\\implies$ elliptical ${\\bf \\Sigma}$. Whitened $\\implies$ spherical ${\\bf \\Sigma}$\n",
    "\n",
    "$${\\bf \\Lambda}^{-1/2} {\\bf \\Lambda} {\\bf \\Lambda}^{-1/2} = {\\bf I}$$\n",
    "$${\\bf \\Lambda}^{-1/2} {\\bf \\Phi}^T {\\bf \\Sigma} {\\bf \\Phi} {\\bf \\Lambda}^{-1/2} = {\\bf I} \\quad\\quad\\quad substituting\\quad in (1)$$\n",
    "\n",
    "- To apply to $\\hat{\\bf x}_i$, multiply by this scale factor $\\rightarrow$ whitened data point $\\tilde{\\bf x}_i$:\n",
    "\n",
    "$$\\tilde{\\bf x}_i = {\\bf \\Lambda}^{-1/2} \\hat{\\bf x}_i = {\\bf \\Lambda}^{-1/2}{\\bf \\Phi}^T{\\bf x}_i \\quad (3)$$\n",
    "\n",
    "- Now ${\\bf \\Sigma}$ is not only diagonal, but also uniform (white), since $E(\\tilde{\\bf x}_i {\\tilde{\\bf x}_i}^T) = {\\bf I}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When this might not be useful\n",
    "1. The scaling of data examples is somehow important in the inference problem you are looking at\n",
    "    - Could use the eigenvalues as an additional set of features to get around this\n",
    "2. Computation:\n",
    "    - you have to compute the covariance matrix Σ, which may be too large to fit inmemory (if you have thousands of features) or take too long to compute;\n",
    "    - secondly the eigenvalue decomposition is $O(n^3)$ (see [mathoverflow](http://mathoverflow.net/questions/62904/) complexity-of-eigenvalue-decomposition)\n",
    "3. Common ML “gotcha’: calculate the scaling factors on the training data, and then you use equations (2) and (3) to apply the same scaling factors to the test data, otherwise you are at risk of over-fitting (you would be using information from the test set in the training process).\n",
    "\n",
    "[Source](http://courses.media.mit.edu/2010fall/mas622j/whiten.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Density Estimation\n",
    "\n",
    "- Non-parametric: histogram, kernel density estimate\n",
    "    + Very flexible\n",
    "    + Little or no prior knowledge required\n",
    "    + Inference is easy\n",
    "    - Expensive in memory and CPU (must store all data)\n",
    "    - Not much opportunity to incorporate prior knowledge\n",
    "- Parametric: Gaussian mixture model\n",
    "    + Restricted family of functions\n",
    "    + Encode assumptions\n",
    "    + Compact representation\n",
    "    - Inflexible; model might be wrong!\n",
    "    - Appropriate model might be obscure, complicated\n",
    "- Semi-parametric: Dirichlet process mixture model (infinite limit of GMM)\n",
    "\n",
    "\n",
    "[http://scikit-learn.org/stable/modules/density.html](http://scikit-learn.org/stable/modules/density.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Mixture Model\n",
    "\n",
    "- Mixture Model $f(x) = \\sum_j \\pi_j g_j(x)\\quad \\mbox{s.t.} \\quad0 \\leq \\pi_j \\leq 1, \\sum_j \\pi_j = 1$\n",
    "- Gaussian mixture: $g_j(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma_j^2}} e^{-(x - \\mu_j)^2 / 2 \\sigma_j^2}$\n",
    "\n",
    "#### EM Alogrithm\n",
    "\n",
    "- Take initial guesses for the parameters\n",
    "- E-step: compute responsibilities\n",
    "- M-step: compute weighted Gaussians\n",
    "- Iterate until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](resources/em_gmm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kernel Density Estimation\n",
    "\n",
    "- Non-parametric way to estimate the probability density function\n",
    "\n",
    "$$\\hat {f}_{h}(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i) = \\frac{1}{nh} \\sum_{i=1}^ n K\\left(\\frac{x - x_i}{h}\\right)$$\n",
    "    where $K(\\cdot)$ is a \\alert{kernel} or smoothing function - see [non-parametric_statistics][1]\n",
    "    \n",
    "[1]: https://en.wikipedia.org/wiki/Kernel_(statistics)#In_non-parametric_statistics    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](resources/KDE_Hist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bivariate and Multivariate Analysis\n",
    "- Samples consists of more than one variable\n",
    "- Descriptive statistics may be used to describe the relationship between pairs of variables\n",
    "    - Cross-tabulations and contingency tables\n",
    "    - Graphical representation via scatter-plots\n",
    "    - Quantitative measures of dependence\n",
    "        - correlation (Pearson’s r if both continuous else Spearman’s rho)\n",
    "        - Covariance (reflects scale)\n",
    "    - Descriptions of conditional distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "- Assume we are given a data set of (high-dimensional) input objects\n",
    "\n",
    "$$\\mathbf{X} = \\{xb_i\\}_{i=1,\\ldots,m}, \\quad xb_i \\in \\mathbb{R}^n$$\n",
    "\n",
    "- Aim is to learn a $k$-dimensional embedding in which each object is represented by a point\n",
    "\n",
    "$$\\mathbf{P} = \\{\\mathbf{p}_i\\}_{i=1,\\ldots,m}, \\quad \\mathbf{p}_i \\in \\mathbb{R}^k$$\n",
    "\n",
    "- typical values for $k$ are 2 or 3 (for visualisation), in general $k << n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Component Analysis (CPA)\n",
    "\n",
    "- Standardise the data\n",
    "- Obtain the eigenvectors and eigenvalues from the covariance matrix or correlation matrix, or perform SVD.\n",
    "- Sort eigenvalues (descending) and choose the $k$ eigenvectors that correspond to the $k$ largest eigenvalues ($k \\leq n$)\n",
    "- Construct the projection matrix $\\mathbf{R}$ from the selected $k$ eigenvectors\n",
    "- Transform the original dataset $\\mathbf{X}$ via $\\mathbf{R}$ to obtain a $k$-dimensional feature subspace $\\mathbf{P}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Projections\n",
    "\n",
    "\n",
    "$$ P = XR $$\n",
    "\n",
    "Where: \n",
    "\n",
    "$$ X \\in \\mathbb{R}^{m \\times n} \\quad {(data\\; matrix)}$$\n",
    "\n",
    "$$ R \\in \\mathbb{R}^{k \\times m} \\quad {(project\\; matrix)}$$\n",
    "\n",
    "$$ P \\in \\mathbb{R}^{m \\times k} \\quad {(lower dimensional represenation)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Random Projections\n",
    "\n",
    "- First row is a random unit vector uniformly chosen from $S^{n-1}$. \n",
    "- Second row is a r.u.v from the space orthogonal to the first row\n",
    "- Third row is a r.u.v. from the space orthogonal to the first two rows, etc. \n",
    "- In this way of choosing $\\mathbf{R}$, the following properties are satisfied:\n",
    "    - Spherical symmetry: For $\\mathbf{A} \\in \\mathcal{O}(n)$, i.e. $\\mathbf{A}\\mathbf{A}^T = \\mathbf{A}^T\\mathbf{A} = \\mathbf{I}$, $\\mathbf{A}\\mathbf{R}$ and $\\mathbf{R}$ have the same distribution.\n",
    "    - Orthogonality: The rows of $\\mathbf{R}$ are orthogonal to each other.\n",
    "    - Normality: The rows of $\\mathbf{R}$ are unit-length vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Johnson-Lindenstrauss Lemma\n",
    "\n",
    "Given $0 < \\varepsilon < 1$, a set $X$ of $m$ points in $\\mathbb{R}^n$, and a number $k > 8 \\frac{\\log(m)}{\\varepsilon^2}$, there is a linear map $f: \\mathbb{R}^n \\mapsto \\mathbb{R}^k$ such that:\n",
    "\n",
    "$$ (1-\\varepsilon )\\|u - v\\|^{2}\\leq \\| f(u) - f(v) \\|^{2}\\leq (1+\\varepsilon ) \\| u - v\\|^{2} $$\n",
    "\n",
    "for all $u, v \\in X$\n",
    "\n",
    "- Proof: see Dasgupta and Gupta (2003)\n",
    "- An orthogonal projection will, in general, reduce the average distance between points\n",
    "    - Roll the dice → random projection\n",
    "    - Scale up the distances so that the average distance is the same\n",
    "- Compatible with approximate nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Database friendly random projections (Achlioptas, 2001)\n",
    "\n",
    "\n",
    "$$\\mathbf{R}_{i,j}={\\sqrt {3}}{\\begin{cases}+1&{\\text{with probability }}{\\frac {1}{6}}\\\\0&{\\text{with probability }}{\\frac {2}{3}}\\\\-1&{\\text{with probability }}{\\frac {1}{6}}\\end{cases}}$$\n",
    "\n",
    "\n",
    "[http://scikit-learn.org/stable/modules/random_projection.html](http://scikit-learn.org/stable/modules/random_projection.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Van Der Maaten et al., 2009)#\n",
    "\n",
    "![alt text](resources/dim_red.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### t-Distributed stochastic neighbour embedding (Maaten and Hinton, 2008)\n",
    "\n",
    "- t-SNE minimises divergence of two distributions over pairwise similarities of:\n",
    "    - input objects $(P_i)$\n",
    "    - corresponding low-dimensional points in the embedding $(Q_i)$\n",
    "- Student-t distribution rather than a Gaussian to compute the similarity between two points in the low-dimensional space\n",
    "- Assume a function that computes a distance between a pair of objects, e.g. Euclidean distance $d(x_i, x_j) = \\left\\|x_i - x_j\\right\\|$\n",
    "- Minimise cost function (KL-divergence) $$C = \\sum_i KL(P_i || Q_i) = \\sum_i \\sum_j p_{j|i} \\log p_{j|i} q_{j|i}$$\n",
    "+ t-SNE compares favourably to other techniques for data visualisation\n",
    "- unclear how t-SNE performs on general dimensionality reduction tasks\n",
    "- relatively local nature of t-SNE makes it sensitive to the curse of the intrinsic dimensionality of the data\n",
    "- not guaranteed to converge to a global optimum of its cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### t-SNE on MNIST\n",
    "\n",
    "![alt text](resources/mnist_tsne.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### t-SNE on MNIST\n",
    "\n",
    "![alt text](resources/mnist_tsne_cropped.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
